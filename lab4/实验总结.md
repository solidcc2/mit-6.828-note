# Lab 4: Preemptive Multitasking
> https://pdos.csail.mit.edu/6.828/2018/labs/lab4/

## 实验内容简介
本实验中，会在同时处于活动的多个用户模式环境（进程）中实现抢占式多任务处理

在Part A，将为JOS添加多处理机支持（多核），实现round-robin调度，并添加基础的环境（进程）管理系统调用

在Part B，实现unix-like fork()，它允许用户环境创建自身的副本

在Part C，将支持进程间通信(IPC)，允许不同的用户模式环境互相通信或同步。还会加入硬件时钟中断处理程序和抢占。

## 实验过程
### Part A: Multiprocessor Support and Cooperative Multitasking

#### Multiprocessor Support
- SMP(对称多处理): 一种多处理机模型，其内所有cpu对系统内资源（内存、IO总线等）有相同的访问权限。
- BSP(bootstrap processor): SMP系统启动过程中负责初始化和启动操作系统的cpu（只有一个，由BIOS和硬件决定）
- AP(application processor): SMP系统中在由BSP激活的cpu
- MMIO(Memory-mapped I/O): 一些硬布线到物理地址空间内的IO设备。
- LAPIC(Local APIC): SMP系统中每个cpu都有的一个部件，负责在系统内传递中断（BSP启动其它AP），为cpu提供唯一id（用于生成cpunum）。LAPIC使用MMIO在32位系统的第二个hole，从高地址0xFE000000(=4GB-32MB)开始（物理内存hole见lab1）
- MMIOBASE: 在JOS的虚拟内存memlayout中，有一个4MB区域被保留给MMIO

#### Exercise 1. 实现一个MMIO映射的分配函数
> Implement mmio_map_region in kern/pmap.c. To see how this is used, look at the beginning of lapic_init in kern/lapic.c. You'll have to do the next exercise, too, before the tests for mmio_map_region will run.

```c
void *
mmio_map_region(physaddr_t pa, size_t size)
{
	// Where to start the next region.  Initially, this is the
	// beginning of the MMIO region.  Because this is static, its
	// value will be preserved between calls to mmio_map_region
	// (just like nextfree in boot_alloc).
	static uintptr_t base = MMIOBASE;

	// Your code here:
	pa = ROUNDDOWN(pa, PGSIZE);
	uintptr_t paend = ROUNDUP(pa+size, PGSIZE);
	size = paend - pa;
	if(base + size >= MMIOLIM){
		panic("out of memory");
	}
	boot_map_region(kern_pgdir, base, size, pa, PTE_PWT|PTE_PCD|PTE_W);
	void* old = (void*)base;
	base += size;
	return old;
	panic("mmio_map_region not implemented");
}
```

#### Application Processor Bootstrap

#### Exercise 2. 理解AP启动流程，修改虚拟内存映射，将MPENTRY_PADDR移除空闲内存池
> Read boot_aps() and mp_main() in kern/init.c, and the assembly code in kern/mpentry.S. Make sure you understand the control flow transfer during the bootstrap of APs. Then modify your implementation of page_init() in kern/pmap.c to avoid adding the page at MPENTRY_PADDR to the free list, so that we can safely copy and run AP bootstrap code at that physical address. Your code should pass the updated check_page_free_list() test (but might fail the updated check_kern_pgdir() test, which we will fix soon).

```c
void
page_init(void)
{
	size_t i;
	for (i = 0; i < npages; i++) {
		if(page2pa(&pages[i])==0 || 
			(page2pa(&pages[i]) >= IOPHYSMEM && page2pa(&pages[i]) < PADDR(envs+NENV)) ||
			(page2pa(&pages[i]) == MPENTRY_PADDR)       // 将MPENTRY_PADDR移除空闲内存池
			){
			pages[i].pp_ref = 1;
			pages[i].pp_link = NULL;
			continue;
		}
		pages[i].pp_ref = 0;
		pages[i].pp_link = page_free_list;
		page_free_list = &pages[i];
	}
}
```

#### Question
1. 比较kern/mpentry.S和boot/boot.S的异同， kern/mpentry.S是被编译并链接到高于KERNBASE的地址空间，宏MPBOOTPHYS的作用是什么？为什么它对kern/mpentry.S是必要的，而boot/boot.S不需要？换句话说，如果kern/mpentry.S忽略了该宏，会出什么错？
> Compare kern/mpentry.S side by side with boot/boot.S. Bearing in mind that kern/mpentry.S is compiled and linked to run above KERNBASE just like everything else in the kernel, what is the purpose of macro MPBOOTPHYS? Why is it necessary in kern/mpentry.S but not in boot/boot.S? In other words, what could go wrong if it were omitted in kern/mpentry.S?
Hint: recall the differences between the link address and the load address that we have discussed in Lab 1.

答：两者的区别主要包含两点：1. boot/boot.S在链接地址和载入地址都是低地址，而kern/mpentry.S被链接在高于KERNBASE的地址区域，在载入时，被载入到MPENTRY_PADDR(低地址);2. boot/boot.S需要负责启动大于A20的地址线，而kern/mpentry.S不需要。
MPBOOTPHYS的作用是将被载入到MPENTRY_PADDR的AP启动用ELF中的虚拟地址(链接地址)根据段内相对偏移和载入地址(MPENTRY_PADDR)转换成物理地址`#define MPBOOTPHYS(s) ((s) - mpentry_start + MPENTRY_PADDR)`，而boot/boot.S因为开始就被载入和链接到低地址，本身使用的就是使用物理地址寻址，无需转换。如果kern/mpentry.S忽略了该宏，则会将所有名称直接使用高地址（虚拟地址）当做物理地址来访存，导致访存失败。
```shell
shell> objdump -h obj/boot/boot.out 

obj/boot/boot.out:     file format elf32-i386

Sections:
Idx Name          Size      VMA       LMA       File off  Algn
  0 .text         0000019f  00007c00  00007c00  00000074  2**2
                  CONTENTS, ALLOC, LOAD, CODE
  1 .eh_frame     000000a8  00007da0  00007da0  00000214  2**2
                  CONTENTS, ALLOC, LOAD, READONLY, DATA
  2 .stab         00000870  00000000  00000000  000002bc  2**2
                  CONTENTS, READONLY, DEBUGGING
  3 .stabstr      00000898  00000000  00000000  00000b2c  2**0
                  CONTENTS, READONLY, DEBUGGING
  4 .comment      00000032  00000000  00000000  000013c4  2**0
                  CONTENTS, READONLY
```

#### Per-CPU State and Initialization
每个cpu的独享资源：
- 每个cpu独享一个内核栈
- 每个cpu独享一个TSS和TSS描述符
- 每个cpu独享一个当前环境（进程）指针
- 每个cpu独享一组系统寄存器

#### Exercise 3. 修改mem_init_mp为每个cpu分配内核栈
> Modify mem_init_mp() (in kern/pmap.c) to map per-CPU stacks starting at KSTACKTOP, as shown in inc/memlayout.h. The size of each stack is KSTKSIZE bytes plus KSTKGAP bytes of unmapped guard pages. Your code should pass the new check in check_kern_pgdir().

kern/pmap.c: mem_init_mp
```c
// 让mem_init_mp为每个cpu都分配一个内核栈
static void
mem_init_mp(void)
{
	// Map per-CPU stacks starting at KSTACKTOP, for up to 'NCPU' CPUs.
	//
	// For CPU i, use the physical memory that 'percpu_kstacks[i]' refers
	// to as its kernel stack. CPU i's kernel stack grows down from virtual
	// address kstacktop_i = KSTACKTOP - i * (KSTKSIZE + KSTKGAP), and is
	// divided into two pieces, just like the single stack you set up in
	// mem_init:
	//     * [kstacktop_i - KSTKSIZE, kstacktop_i)
	//          -- backed by physical memory
	//     * [kstacktop_i - (KSTKSIZE + KSTKGAP), kstacktop_i - KSTKSIZE)
	//          -- not backed; so if the kernel overflows its stack,
	//             it will fault rather than overwrite another CPU's stack.
	//             Known as a "guard page".
	//     Permissions: kernel RW, user NONE
	//
	// LAB 4: Your code here:
	uintptr_t base = KSTACKTOP;
	for(uint32_t i=0; i<NCPU; i++){
		boot_map_region(kern_pgdir, (uintptr_t)(base-KSTKSIZE), KSTKSIZE, PADDR(percpu_kstacks[i]), PTE_W);
		base -= KSTKSIZE;
		base -= KSTKGAP;
	}
}
```

kern/pmap.c: mem_init

系统启动过程中定义在kern/entry.S的.data的栈作为启动栈，系统正常启动后，切换到用户进程，再每次由用户进程陷入到内核时，根据tss中注册的cpu内核栈地址是KSTACKTOP区域的虚拟地址，而原本高于KERNBASE的启动栈不再使用，因此可以不用为其在页表中映射。
```c
    // 取消原来为BSP分配的栈，统一使用mem_init_mp分配内核栈
	// boot_map_region(kern_pgdir, (uintptr_t)(KSTACKTOP-KSTKSIZE), KSTKSIZE, PADDR(bootstack), PTE_W);
	mem_init_mp();
```
#### Exercise 4. 修改trap_init_percpu为每个cpu初始化tss资源
> The code in trap_init_percpu() (kern/trap.c) initializes the TSS and TSS descriptor for the BSP. It worked in Lab 3, but is incorrect when running on other CPUs. Change the code so that it can work on all CPUs. (Note: your new code should not use the global ts variable any more.)

kern/trap.c: trap_init_percpu
```c
void
trap_init_percpu(void)
{
	// The example code here sets up the Task State Segment (TSS) and
	// the TSS descriptor for CPU 0. But it is incorrect if we are
	// running on other CPUs because each CPU has its own kernel stack.
	// Fix the code so that it works for all CPUs.
	//
	// Hints:
	//   - The macro "thiscpu" always refers to the current CPU's
	//     struct CpuInfo;
	//   - The ID of the current CPU is given by cpunum() or
	//     thiscpu->cpu_id;
	//   - Use "thiscpu->cpu_ts" as the TSS for the current CPU,
	//     rather than the global "ts" variable;
	//   - Use gdt[(GD_TSS0 >> 3) + i] for CPU i's TSS descriptor;
	//   - You mapped the per-CPU kernel stacks in mem_init_mp()
	//   - Initialize cpu_ts.ts_iomb to prevent unauthorized environments
	//     from doing IO (0 is not the correct value!)
	//
	// ltr sets a 'busy' flag in the TSS selector, so if you
	// accidentally load the same TSS on more than one CPU, you'll
	// get a triple fault.  If you set up an individual CPU's TSS
	// wrong, you may not get a fault until you try to return from
	// user space on that CPU.
	//
	// LAB 4: Your code here:

	// Setup a TSS so that we get the right stack
	// when we trap to the kernel.
	thiscpu->cpu_ts.ts_esp0 = KSTACKTOP - cpunum()*(KSTKSIZE+KSTKGAP);
	thiscpu->cpu_ts.ts_ss0 = GD_KD;
	thiscpu->cpu_ts.ts_iomb = sizeof(struct Taskstate);

	// Initialize the TSS slot of the gdt.
	gdt[(GD_TSS0 >> 3) + cpunum()] = SEG16(STS_T32A, (uint32_t) (&thiscpu->cpu_ts),
					sizeof(struct Taskstate) - 1, 0);
	gdt[(GD_TSS0 >> 3) + cpunum()].sd_s = 0;

	// Load the TSS selector (like other segment selectors, the
	// bottom three bits are special; we leave them 0)
	ltr(GD_TSS0 + 8*cpunum());

	// Load the IDT
	lidt(&idt_pd);
}
```

#### Locking
每个AP在mp_main初始化后，目前是停下来轮询自己的状态，此时还未进入内核的公共区域。为了让它们能够进入公共区域，最重要的问题是解决公共资源竞争。最简单的方法是加内核大锁，让所有AP在进入内核时先要获取锁。

需要加锁的位置：
- 在i386_init()，在BSP唤醒其他AP前需要获取锁
- 在mp_main()，在初始化AP后，需要获取锁才能在该AP上调用sched_yield()进入环境（进程）
- 在trap()，当从用户模式陷入时需要获取锁。需要根据tf_cs的最低2位来判断环境(进程)处于内核态还是用户态。
- 在env_run()，需要在切换到用户态前释放锁。不能太早也不能太迟，否则会产生竞争甚至死锁。

#### Exercise 5. 在每个内核入口加锁，每个内核出口解锁
> Apply the big kernel lock as described above, by calling lock_kernel() and unlock_kernel() at the proper locations.

kern/init.c: i386_init
在启动AP前加锁
```c
void
i386_init(void)
{
	// Initialize the console.
	// Can't call cprintf until after we do this!
	cons_init();

	cprintf("6828 decimal is %o octal!\n", 6828);

	mem_init();

	env_init();
	trap_init();

	mp_init();
	lapic_init();

	pic_init();

	// Acquire the big kernel lock before waking up APs
	// Your code here:
	lock_kernel();  // 在启动AP前为内核加锁
	
	// Starting non-boot CPUs
	boot_aps();

#if defined(TEST)
	// Don't touch -- used by grading script!
	ENV_CREATE(TEST, ENV_TYPE_USER);
#else
	// Touch all you want.
	ENV_CREATE(user_yield, ENV_TYPE_USER);
	ENV_CREATE(user_yield, ENV_TYPE_USER);
	ENV_CREATE(user_yield, ENV_TYPE_USER);
#endif // TEST*

	// Schedule and run the first user environment!
	sched_yield();
}
```
kern/init.c: mp_main

在给AP调度用户环境（进程）前加锁
```c
void
mp_main(void)
{
	// We are in high EIP now, safe to switch to kern_pgdir 
	lcr3(PADDR(kern_pgdir));
	cprintf("SMP: CPU %d starting\n", cpunum());

	lapic_init();
	env_init_percpu();
	trap_init_percpu();
	xchg(&thiscpu->cpu_status, CPU_STARTED); // tell boot_aps() we're up

	// Now that we have finished some basic setup, call sched_yield()
	// to start running processes on this CPU.  But make sure that
	// only one CPU can enter the scheduler at a time!
	//
	// Your code here:
	lock_kernel();  // 在给AP调度用户环境（进程）前加锁
	sched_yield();
	// Remove this after you finish Exercise 6
	for (;;);
}
```
kern/trap.c: trap
```c
用户态陷入，需要加锁
void
trap(struct Trapframe *tf)
{
	// The environment may have set DF and some versions
	// of GCC rely on DF being clear
	asm volatile("cld" ::: "cc");

	// Halt the CPU if some other CPU has called panic()
	extern char *panicstr;
	if (panicstr)
		asm volatile("hlt");

	// Re-acqurie the big kernel lock if we were halted in
	// sched_yield()
	if (xchg(&thiscpu->cpu_status, CPU_STARTED) == CPU_HALTED)
		lock_kernel();
	// Check that interrupts are disabled.  If this assertion
	// fails, DO NOT be tempted to fix it by inserting a "cli" in
	// the interrupt path.
	assert(!(read_eflags() & FL_IF));

	if ((tf->tf_cs & 3) == 3) {
		// Trapped from user mode.
		// Acquire the big kernel lock before doing any
		// serious kernel work.
		// LAB 4: Your code here.
		lock_kernel();      // 从用户态陷入，需要加锁
		assert(curenv);
```
kern/env.c: env_run

在内核栈退栈到用户态前解锁
```c
void
env_run(struct Env *e)
{
	// Step 1: If this is a context switch (a new environment is running):
	//	   1. Set the current environment (if any) back to
	//	      ENV_RUNNABLE if it is ENV_RUNNING (think about
	//	      what other states it can be in),
	//	   2. Set 'curenv' to the new environment,
	//	   3. Set its status to ENV_RUNNING,
	//	   4. Update its 'env_runs' counter,
	//	   5. Use lcr3() to switch to its address space.
	// Step 2: Use env_pop_tf() to restore the environment's
	//	   registers and drop into user mode in the
	//	   environment.

	// Hint: This function loads the new environment's state from
	//	e->env_tf.  Go back through the code you wrote above
	//	and make sure you have set the relevant parts of
	//	e->env_tf to sensible values.

	// LAB 3: Your code here.
	if(curenv != NULL){
		if(curenv->env_status == ENV_RUNNING){
			curenv->env_status = ENV_RUNNABLE;
		}
	}
	curenv = e;
	curenv->env_status = ENV_RUNNING;
	curenv->env_runs++;
	lcr3(PADDR(e->env_pgdir));
	unlock_kernel();    // 在内核栈退栈到用户态前解锁
	env_pop_tf(&(e->env_tf));
	
	panic("env_run not yet implemented");
}
```

#### Question
2. 内核大锁死活保证了同时只会有一个CPU能够进入内核，为什么我们仍要为每个CPU提供一个内核栈？描述在什么情况下使用共享内核栈即使使用内核大锁也会出错？
> It seems that using the big kernel lock guarantees that only one CPU can run the kernel code at a time. Why do we still need separate kernel stacks for each CPU? Describe a scenario in which using a shared kernel stack will go wrong, even with the protection of the big kernel lock.

答：考虑有2个进程env1, env2同时陷入内核，使用共享内核栈，则在到达trap的锁前，他们各自的处理器会同时自动向内核栈压栈自己的状态，导致trapframe出错。

#### Round-Robin Scheduling

#### Exercise 6. 修改sched_yield来实现round-robin调度方案，同时要补充sys_yield系统调用

kern/sched.c: sched_yield

round robin调度器
```c
void
sched_yield(void)
{
	struct Env *idle;

	// Implement simple round-robin scheduling.
	//
	// Search through 'envs' for an ENV_RUNNABLE environment in
	// circular fashion starting just after the env this CPU was
	// last running.  Switch to the first such environment found.
	//
	// If no envs are runnable, but the environment previously
	// running on this CPU is still ENV_RUNNING, it's okay to
	// choose that environment.
	//
	// Never choose an environment that's currently running on
	// another CPU (env_status == ENV_RUNNING). If there are
	// no runnable environments, simply drop through to the code
	// below to halt the cpu.

	// LAB 4: Your code here.
	struct Env *e;
	// cprintf("curenv: %x\n", curenv);
	int i, cur=0;
	if (curenv) cur=ENVX(curenv->env_id);
    else cur = 0;
	// cprintf("cur: %x, thiscpu: %x\n", cur, thiscpu->cpu_id);
	for (i = 0; i < NENV; ++i) {
		int j = (cur+i) % NENV;
		// if (j < 2) cprintf("envs[%x].env_status: %x\n", j, envs[j].env_status);
		if (envs[j].env_status == ENV_RUNNABLE) {
			if (j == 1) 
				cprintf("\n");
			env_run(envs + j);
		}
	}
	if (curenv && curenv->env_status == ENV_RUNNING)
		env_run(curenv);
	// sched_halt never returns
	sched_halt();
}
```

kern/syscall.c: syscall

注册sys_yield系统调用
```c
	case SYS_yield:
		sys_yield();
		return 0;
```

#### Questions
3. 在env_run的实现中，调用了lcr3()。在lcr3()调用前后代码对指针e（env_run的参数）寻址。在载入%cr3寄存器后，MMU使用的寻址环境立刻被切换。但是虚拟地址e在新的环境中依旧有意义。为什么指针e能在页目录切换前后都能被正确解引用？
> In your implementation of env_run() you should have called lcr3(). Before and after the call to lcr3(), your code makes references (at least it should) to the variable e, the argument to env_run. Upon loading the %cr3 register, the addressing context used by the MMU is instantly changed. But a virtual address (namely e) has meaning relative to a given address context--the address context specifies the physical address to which the virtual address maps. Why can the pointer e be dereferenced both before and after the addressing switch?

答：因为e指针实际上是在KERNBASE之上的虚拟地址，可被kern_pgdir解析，而用户环境的页表是从kern_pgdir中拷贝来的，对KERNBASE以上的地址有相同的映射，且环境还处于内核态，所以可以解析。

4. 只要内核从一个环境(进程)切换到另一个，它必须保证旧环境(进程)的寄存器都被保存起来，以便于后面能正确的恢复旧环境。为什么？这个过程发生在哪？
> Whenever the kernel switches from one environment to another, it must ensure the old environment's registers are saved so they can be restored properly later. Why? Where does this happen?

答：在用户进程陷入内核时，kern/trap.c: trap函数会保存TrapFrame, `curenv->env_tf = *tf`。

#### System Calls for Environment Creation
与环境(进程)创建相关的系统调用:
- sys_exofork: 
创建一个几乎空白的进程，地址空间内没有映射任何用户区域且不可运行。新进程有与父进程相同的寄存器状态。父进程中sys_exofork返回子进程的envid_t(创建失败返回负的错误码)。子进程中返回0（但因为子进程不可运行，所以实际调用时不会返回，直到改变状态）
- sys_env_set_status: 
将指定的环境状态设置为ENV_RUNNABLE或ENV_NOT_RUNNABLE。该系统调用通常用于在其地址空间和寄存器状态完全初始化后标记新环境已准备好。
- sys_page_alloc: 
为给定的环境(进程)分配物理页，并映射到其地址空间。
- sys_page_map: 
从一个环境的地址空间中拷贝地址映射（不拷贝内容）到另一个环境，维护内存原地共享，从而是新旧映射都指向同一块物理内存。
- sys_page_unmap:
将给定虚拟地址从给定环境中解除映射。

#### Exercise 7. 实现上述系统调用
> Implement the system calls described above in kern/syscall.c and make sure syscall() calls them. You will need to use various functions in kern/pmap.c and kern/env.c, particularly envid2env(). For now, whenever you call envid2env(), pass 1 in the checkperm parameter. Be sure you check for any invalid system call arguments, returning -E_INVAL in that case. Test your JOS kernel with user/dumbfork and make sure it works before proceeding.

见kern/syscall.c

kern/syscall.c: syscall

注册系统调用
```c
	case SYS_env_set_status:
		return sys_env_set_status(a1, a2);
	case SYS_exofork:
		return sys_exofork();
	case SYS_page_alloc:
		return sys_page_alloc(a1, (void*)a2, a3);
	case SYS_page_unmap:
		return sys_page_unmap(a1, (void*)a2);
	case SYS_page_map:
		return sys_page_map(a1, (void*)a2, a3, (void*)a4, a5);
```

### Part B: Copy-on-Write Fork

#### Exercise 8. 实现用户级别缺页处理
> Implement the sys_env_set_pgfault_upcall system call. Be sure to enable permission checking when looking up the environment ID of the target environment, since this is a "dangerous" system call.

定义钩子系统调用`sys_env_set_pgfault_upcall`，允许用户程序设置缺页时的处理函数
```c
static int
sys_env_set_pgfault_upcall(envid_t envid, void *func)
{
	// LAB 4: Your code here.
	struct Env * e;
	int r = envid2env(envid, &e, 1);
	if(r < 0) return -E_BAD_ENV;
	e->env_pgfault_upcall = func;
	return 0;
	panic("sys_env_set_pgfault_upcall not implemented");
}

kern/syscall.c: syscall

注册系统调用
```c
	case SYS_env_set_pgfault_upcall:
		return sys_env_set_pgfault_upcall(a1, (void*)a2);
```

#### Normal and Exception Stacks in User Environments
用户空间的普通栈与异常栈

用户程序通常工作在普通的用户栈上，保存在虚拟地址空间的`USTACKTOP-PGSIZE`到`USTACKTOP-1`。当在用户模式发生缺页时，内核将在异常栈（保存在`UXSTACKTOP-PGSIZE`到`UXSTACKTOP-1`)上重启用户环境，并运行指定的用户级页面错误处理程序（由`sys_env_set_pgfault_upcall`指定）。

#### Invoking the User Page Fault Handler
调用用户缺页异常处理的过程：
- 用户环境发生缺页
- 用户环境陷入内核
- 内核在用户环境异常栈上初始化`UTrapFrame`，它包含了后续恢复场景的所有信息<br>
(如果用户环境没有通过`sys_env_set_pgfault_upcall`系统调用指定缺页处理程序则直接销毁用户环境)
- 内核安排用户环境在异常栈上继续执行用户缺页处理程序
- 完成缺页处理后，通过精心构造的栈帧及汇编指令，退栈ret回页错误点，应用处于`UTrapFrame`中的`%esp`, `%ebp`, `%eip`，从而返回普通用户栈上继续执行用户程序。

注：页面错误处理程序本身出错，则只需在当前异常栈上继续建立一个新的`UTrapFrame`（在tf->tf_esp处而不是UXSTACKTOP处，同时需要先压入一个空32位字再压入UTrapFrame, 因为ret返回时需要留一个字的空间用于向原栈中压入eip，具体见后）

#### Exercise 9. 实现上述栈切换过程
> Implement the code in page_fault_handler in kern/trap.c required to dispatch page faults to the user-mode handler. Be sure to take appropriate precautions when writing into the exception stack. (What happens if the user environment runs out of space on the exception stack?)

```c
void
page_fault_handler(struct Trapframe *tf)
{
	uint32_t fault_va;

	fault_va = rcr2();  // 读取cr2寄存器状态（cr2保存缺页错误的虚拟地址）

	if ((tf->tf_cs & 3) == 0)   // 暂时不管内核缺页错误
		panic("page_fault_handler():page fault in kernel mode!\n");

	if(curenv->env_pgfault_upcall){
		struct UTrapframe* t;
        // 结构体存放时的地址还是向上增长，而栈是向下增长，计算结构体起始位置
		if(tf->tf_esp >= UXSTACKTOP-PGSIZE && tf->tf_esp < UXSTACKTOP){
			t = (struct UTrapframe*)(tf->tf_esp - sizeof(struct UTrapframe) - 4);   // 当已经处于异常栈时，需要压入一个额外的32位字

		}else{
			t = (struct UTrapframe*)(UXSTACKTOP - sizeof(struct UTrapframe));
		}
		user_mem_assert(curenv, t, sizeof(*t), PTE_U);  // 确认UTrapframe在用户空间可访问
        // 填入异常发生时的状态（也是将来返回用户栈的信息）
		t->utf_esp = tf->tf_esp;    // 用户栈栈帧
		t->utf_eflags = tf->tf_eflags; // 缺页发生时的eflags 
		t->utf_eip = tf->tf_eip;    // 缺页发生时的eip（iret返回地址）
		t->utf_regs = tf->tf_regs;  // 缺页发生时的寄存器
		t->utf_err = tf->tf_err;    // 缺页发生时的error code
		t->utf_fault_va = fault_va; // 导致缺页的虚拟地址
		
        // 要从内核态返回用户态，但是不再返回缺页位置，而是%eip进入缺页异常处理代码入口
        // %esp切换为异常栈栈帧UTrapframe起始位置
		tf->tf_eip = (uintptr_t)curenv->env_pgfault_upcall;     
		tf->tf_esp = (uintptr_t)t;
		env_run(curenv);    // 切换回用户态
	}	
	// Destroy the environment that caused the fault.
	cprintf("[%08x] user fault va %08x ip %08x\n",
		curenv->env_id, fault_va, tf->tf_eip);
	print_trapframe(tf);    
	env_destroy(curenv);    // 如果没有指定缺页处理程序，则直接销毁环境
}
```

#### User-mode Page Fault Entrypoint
#### Exercise 10. 实现从异常栈返回通用用户栈
> Implement the _pgfault_upcall routine in lib/pfentry.S. The interesting part is returning to the original point in the user code that caused the page fault. You'll return directly there, without going back through the kernel. The hard part is simultaneously switching stacks and re-loading the EIP.
```c
.text
.globl _pgfault_upcall
_pgfault_upcall:
	// Call the C page fault handler.
	pushl %esp			// 当前异常栈中%esp即指向UStackframe，因此该指针即是
                        // 压入_pgfault_handler的参数
	movl _pgfault_handler, %eax 
	call *%eax          // 调用_pgfault_handler
	addl $4, %esp		// 栈向下生长，%esp+4即是弹出_pgfault_handler的参数

	// LAB 4: Your code here.
	subl $0x4, 0x30(%esp)   // 0x30(%esp)指向UStackframe的utf_esp成员，
                            // 也即也错误发生时的栈指针，将其减4
                            // 也就是在原栈上执行压栈的第一步，esp下移。
                            // 这即是当在UXStack发生也错误时，需要额外压入
                            // 4字节(一个字)空白空间的原因
	movl 0x28(%esp), %ecx   // 0x28(%esp)指向页错误时的%eip
	movl 0x30(%esp), %ebx   
	movl %ecx, (%ebx)       // 完成向原栈压栈的第二步，填入错误发生时的返回%eip
                            // 到此在原栈里构造出了一个能够被ret命令返回的结构
                            // （ret命令会读取%esp的内容到%eip再退栈%esp+=4)

	// LAB 4: Your code here.
	addl $0x8, %esp         // %esp+8，使%esp指向UTrapframe.utf_regs
	popal		            // popal 从栈中退栈载入所有寄存器
                            // popal 后%esp仅仅减了0x20，没有被覆盖,该命令不会
                            // 应用栈中保存的esp到%esp

	// LAB 4: Your code here.
	addl $0x4, %esp         // %esp+4，跳过eip使%esp指向UTrapframe.e_eflags
	popfl                   // popfl 退栈并载入也错误点的eflags

	// LAB 4: Your code here.
	popl %esp               // 此时%esp指向UTrapframe.utf_esp，退栈载入%esp，
                            // 此时%esp指向原栈中构造的可以通过ret返回页错误点
                            // 的位置

	// Return to re-execute the instruction that faulted.
	// LAB 4: Your code here.
	ret                     // 返回到页错误点
```
参考UTrapframe结构
```c
struct UTrapframe {
	/* information about the fault */
	uint32_t utf_fault_va;	/* va for T_PGFLT, 0 otherwise */   // 栈顶（低地址）
	uint32_t utf_err;
	/* trap-time return state */
	struct PushRegs utf_regs;   // 大小8个字，32个字节
	uintptr_t utf_eip;
	uint32_t utf_eflags;
	/* the trap-time stack to return to */
	uintptr_t utf_esp;                                          // 栈底（高地址）
} __attribute__((packed));
```


#### Exercise 11. 完成用户级页面错误处理的C库
> Finish set_pgfault_handler() in lib/pgfault.c.
```c
void
set_pgfault_handler(void (*handler)(struct UTrapframe *utf))
{
	int r;
	if (_pgfault_handler == 0) {
		// First time through!
		// LAB 4: Your code here.
		r = sys_page_alloc(0, (void*)UXSTACKTOP-PGSIZE, PTE_U|PTE_W);   // 分配异常栈
		if(r < 0){
			panic("sys_page_alloc fail, %e", r);
		}
		r = sys_env_set_pgfault_upcall(0, _pgfault_upcall); // 将_pgfault_upcall设置为也错误处理程序。
                                                            // 注意：不是handler, handler只有页处理逻辑,而_pgfault_upcall会调用handler，且_pgfault_upcall包含返回用户栈所必要的汇编指令
		if(r < 0){
			panic("sys_env_set_pgfault_upcall fail, %e", r);
		}

		// panic("set_pgfault_handler not implemented");
	}

	// Save handler pointer for assembly to call.
	_pgfault_handler = handler;                             // 设置_pgfault_upcall调用handler
}
```

#### Implementing Copy-on-Write Fork
fork的主要控制流：
1. 父进程使用set_pgfault_handler()安装pgfault()作为C-level页错误处理程序。
2. 父进程调用sys_exofork()创建子进程。
3. 对地址空间UTOP之下每个可写页，或写时复制页，父进程调用duppage(duppage会将该页map为子进程地址空间的写时复制页，在父进程自身地址空间中将该页remap为写时复制页。duppage通过设置PTE为不可写，并在`avail`字段中设置PTE_COW来区分真正的只读页与写时复制页)。对只读页直接map到子进程。
<br>**问:** 一定要先map子进程再map父进程为COW，为什么？
<br>**答:** 因为父进程在调用duppage的过程是在用户态，如果先将自身可写页map为写时复制页，会导致在为子进程map前，duppage函数中的栈上空间不可写，直接触发缺页异常，这不是我们希望看到的。同时，也可能在子进程map前遭遇外部中断修改父进程地址空间。
<br>**注:** 异常栈不会像上述方式map，而是直接为子进程分配新的页作为异常栈，并拷贝其中内容。因为缺页错误处理函数会在异常栈上做实际的页拷贝操作。而异常栈内容的拷贝操作应当由`fork()`进行。

4. 父进程将子进程的页错误入口点设置的和自身一样。
5. 父进程将子进程标记为可运行。

当向COW(写时复制)页写操作时，会产生页错误，页错误处理流程:
1. 内核传递页错误到`_pgfault_upcall`，再传递给`fork`的`pgfault()`。
2. pgfault()先在error code中检查FEC_WR位，判断是写错误，再检查PTE的PTE_COW确认页被标记为PTE_COW写时复制；否则panic。
3. pgfault()分配一个新页映射到临时位置，并拷贝原页内容到其中。然后再将该临时页标记为可写重map到原页虚拟地址。

#### Exercise 12. 实现lib/fork.c中的fork，duppage，pgfault
lib/fork.c: fork
<br>写时复制fork
```c
envid_t
fork(void)
{
	// LAB 4: Your code here.
	set_pgfault_handler(pgfault);   // 设置用户页错误处理函数
	envid_t envid = sys_exofork();  // 创建新进程
	if(envid < 0){
		// err
		panic("sys_exofork");
		return envid;
	}
	if(envid == 0){
		// child
		thisenv = &envs[ENVX(sys_getenvid())];  // 子进程可运行后设置thisenv然后直接返回0
		return 0;
	}
	// parent
	for(uintptr_t va = 0; va < USTACKTOP; va += PGSIZE){
		pde_t pde = uvpd[PDX(va)];
		if(!(pde & PTE_P))  // 页表存在
			continue;
		pte_t pte = uvpt[PGNUM(va)];
		if(!((pte & PTE_P) && (pte & PTE_U)))   // 页存在，且是用户态可访问页
			continue;
		int r;
		if((r = duppage(envid, PGNUM(va))) < 0){    // 对页进行重映射
			int e;
			if((e = sys_env_destroy(envid)) < 0)    // 页重映射失败，则销毁子进程
				panic("sys_env_destory failed, %e", e);
			return r;
		}
	}
    // 为异常栈分配页，并拷贝（目前的测试情况中不存在在异常栈中发生fork的行为，也即异常栈
    // 通常为空，直接分配空异常栈，而不拷贝，也能通过全部测试样例）
	int r = sys_page_alloc(0, (void*)PFTEMP, PTE_W|PTE_U|PTE_P);
	if(r < 0){
		panic("sys_page_alloc failed, %e", r);
	}
	memcpy((void*)PFTEMP, (void*)ROUNDDOWN(UXSTACKTOP-PGSIZE, PGSIZE), PGSIZE);
	r = sys_page_map(0, (void*)PFTEMP, envid, (void*)ROUNDDOWN(UXSTACKTOP-PGSIZE, PGSIZE), PTE_W|PTE_U|PTE_P);
	if(r < 0){
		panic("sys_page_map failed, %e", r);
	}
	if (sys_page_unmap(0, (void*)PFTEMP) < 0)
		panic("sys_page_unmap");

	if(r < 0)
		panic("sys_page_alloc failed, %e", r);
	extern void _pgfault_upcall(void);
	r = sys_env_set_pgfault_upcall(envid, _pgfault_upcall); // 设置页错误处理入口
	if(r < 0)
		panic("sys_env_pgfault_upcall failed, %e", r);
	sys_env_set_status(envid, ENV_RUNNABLE);    // 标示进程可运行
	return envid;                               // 返回子进程id
	panic("fork not implemented");
}
```

lib/fork.c: duppage
<br>为父子进程完成必要的页重映射
```c
static int
duppage(envid_t envid, unsigned pn)
{
	int r;
	// LAB 4: Your code here.
	pte_t pte = uvpt[pn];
	void* addr = (void*)(pn*PGSIZE);
	if((pte & PTE_W) || (pte & PTE_COW)){
		sys_page_map(0, addr, envid, addr, PTE_COW|PTE_U|PTE_P);
		sys_page_map(0, addr, 0, addr, PTE_COW|PTE_U|PTE_P);
	}else{
		sys_page_map(0, addr, envid, addr, PTE_U|PTE_P);
	}
	return 0;
	panic("duppage not implemented");
}
```

lib/fork.c: pgfault
```c
static void
pgfault(struct UTrapframe *utf)
{
	void *addr = (void *) utf->utf_fault_va;
	uint32_t err = utf->utf_err;
	int r;

	// LAB 4: Your code here.
	if(!(err & FEC_WR))     // 检查错误码确认是写错误
		panic("not a write fault");

	if(!((uvpd[PDX(addr)] & PTE_P) &&
		(uvpt[PGNUM(addr)] & PTE_P) &&
		(uvpt[PGNUM(addr)] & PTE_COW))) // 确认是COW页
		panic("not a cow page");

	// LAB 4: Your code here.
	r = sys_page_alloc(0, (void*)PFTEMP, PTE_W|PTE_U|PTE_P); // 分配新页到虚拟地址PFTEMP
	if(r < 0){
		panic("sys_page_alloc failed, %e", r);
	}
	memcpy((void*)PFTEMP, (void*)ROUNDDOWN(addr, PGSIZE), PGSIZE);  // 从拷贝原内容到新页(PFTEMP)
	r = sys_page_map(0, (void*)PFTEMP, 0, (void*)ROUNDDOWN(addr, PGSIZE), PTE_W|PTE_U|PTE_P);   // 将新页PFTEMP映射到原位置，并标记PTE_W，替换原页
	if(r < 0){
		panic("sys_page_map failed, %e", r);
	}
	if (sys_page_unmap(0, (void*)PFTEMP) < 0)   // 取消临时映射PFTEMP
		panic("sys_page_unmap");
	return;
	panic("pgfault not implemented");
}
```

### Part C: Preemptive Multitasking and Inter-Process communication (IPC)

#### Interrupt discipline
外部中断通常称为IRQ。JOS中包含16个可能的IRQ，0-15。IRQ号到IDT号的映射不是固定的，但本系统中将`IRQ` `0`到`15`映射为`IDT`的`IRQ_OFFSET`到`IRQ_OFFSET+15`。其中`IRQ_OFFSET`被定义为`32`，这是为了防止与处理器异常产生冲突（早期MS-DOS，曾将`IRQ_OFFSET`设置为0，就造成了很大的麻烦）

JOS中仅在用户态开启外部中断，在内核中关闭外部中断。外部中断的开关由`%eflags`寄存器的`FL_IF`位决定

#### Exercise 13. 修改代码提供外部中断机制
> Modify kern/trapentry.S and kern/trap.c to initialize the appropriate entries in the IDT and provide handlers for IRQs 0 through 15. Then modify the code in env_alloc() in kern/env.c to ensure that user environments are always run with interrupts enabled.
>
> Also uncomment the sti instruction in sched_halt() so that idle CPUs unmask interrupts.

kern/trapentry.S
<br>定义函数，绑定IDT，注意所有的IRQ都不会写入error code
```c
	TRAPHANDLER_NOEC(irq0, 32)
	TRAPHANDLER_NOEC(irq1, 33)
	TRAPHANDLER_NOEC(irq2, 34)
	TRAPHANDLER_NOEC(irq3, 35)
	TRAPHANDLER_NOEC(irq4, 36)
	TRAPHANDLER_NOEC(irq5, 37)
	TRAPHANDLER_NOEC(irq6, 38)
	TRAPHANDLER_NOEC(irq7, 39)
	TRAPHANDLER_NOEC(irq8, 40)
	TRAPHANDLER_NOEC(irq9, 41)
	TRAPHANDLER_NOEC(irq10, 42)
	TRAPHANDLER_NOEC(irq11, 43)
	TRAPHANDLER_NOEC(irq12, 44)
	TRAPHANDLER_NOEC(irq13, 45)
	TRAPHANDLER_NOEC(irq14, 46)
	TRAPHANDLER_NOEC(irq15, 47)
```

kern/trap.c: trap_init
<br>使用SETGATE宏在IDT中注册中断向量。
<br>**注：**还要修改所有异常和软中断的SETGATE设置，将第二个参数istrap改为0，istrap决定在进入中断时是否修改IF以屏蔽中断。因为JOS目前设计为内核中屏蔽中断
```c
	void irq0();
	void irq1();
	void irq2();
	void irq3();
	void irq4();
	void irq5();
	void irq6();
	void irq7();
	void irq8();
	void irq9();
	void irq10();
	void irq11();
	void irq12();
	void irq13();
	void irq14();
	void irq15();

	SETGATE(idt[32], 0, GD_KT, irq0, 0);
	SETGATE(idt[33], 0, GD_KT, irq1, 0); 
	SETGATE(idt[34], 0, GD_KT, irq2, 0);
	SETGATE(idt[35], 0, GD_KT, irq3, 0);
	SETGATE(idt[36], 0, GD_KT, irq4, 0);
	SETGATE(idt[37], 0, GD_KT, irq5, 0);
	SETGATE(idt[38], 0, GD_KT, irq6, 0);
	SETGATE(idt[39], 0, GD_KT, irq7, 0);
	SETGATE(idt[40], 0, GD_KT, irq8, 0);
	SETGATE(idt[41], 0, GD_KT, irq9, 0);
	SETGATE(idt[42], 0, GD_KT, irq10, 0);
	SETGATE(idt[43], 0, GD_KT, irq11, 0);
	SETGATE(idt[44], 0, GD_KT, irq12, 0);
	SETGATE(idt[45], 0, GD_KT, irq13, 0);
	SETGATE(idt[46], 0, GD_KT, irq14, 0);
	SETGATE(idt[47], 0, GD_KT, irq15, 0);
```

#### Exercise 14. 处理时钟中断
> Modify the kernel's trap_dispatch() function so that it calls sched_yield() to find and run a different environment whenever a clock interrupt takes place.
kern/trap.c: trap_dispatch
```c
	if(tf->tf_trapno == IRQ_OFFSET + IRQ_TIMER){    // 时钟中断
		lapic_eoi();    // 确认时钟中断
		sched_yield();  // 发起调度（调度程序不会返回）
	}
```

#### Exercise 15. 进程间通信(IPC)
> Implement sys_ipc_recv and sys_ipc_try_send in kern/syscall.c. Read the comments on both before implementing them, since they have to work together. When you call envid2env in these routines, you should set the checkperm flag to 0, meaning that any environment is allowed to send IPC messages to any other environment, and the kernel does no special permission checking other than verifying that the target envid is valid.
> 
> Then implement the ipc_recv and ipc_send functions in lib/ipc.c.

kern/syscall.c: sys_ipc_try_send
<br> 尝试send一个值，并把当前地址空间的一个页映射到接收端空间
```c
static int
sys_ipc_try_send(envid_t envid, uint32_t value, void *srcva, unsigned perm)
{
	// LAB 4: Your code here.
	struct Env* e;
	int r;
	r = envid2env(envid, &e, 0);    // 获取接收端环境
	if (r < 0)
		return r;
	if(!e->env_ipc_recving){        // 如果接收端不在接收，则出错
		return -E_IPC_NOT_RECV;
	}
	// perm -- PTE_U | PTE_P must be set, PTE_AVAIL | PTE_W may or may not be set,
//         but no other bits may be set.  See PTE_SYSCALL in inc/mmu.h.
	if(srcva < (void*)UTOP && e->env_ipc_dstva < (void*)UTOP){  // 发送端和接收端的虚拟地址都是用户地址
		if((uintptr_t)srcva % PGSIZE){
			return -E_INVAL;
		}
		pte_t* pte;
		struct PageInfo* pp = page_lookup(curenv->env_pgdir, srcva, &pte);  // 转换发送端虚拟页地址为物理页索引
		if((*pte & perm) != perm){  // 检查pte权限
			return -E_INVAL;
		}
		if(perm & PTE_W &&
			!(*pte & PTE_W)){
				return -E_INVAL;
		}
		
		r = page_insert(e->env_pgdir, pp, e->env_ipc_dstva, perm);  // 将物理页插入接收端的接收虚拟地址
		if(r<0)
			return r;
		e->env_ipc_perm = perm;
	}else{
		e->env_ipc_perm = 0;
	}
    // 对接收端IPC状态进行更新
	e->env_ipc_recving = 0; 
	e->env_ipc_from = curenv->env_id;
	e->env_ipc_value = value;   // 填充IPC发送的单值
	e->env_status = ENV_RUNNABLE;
	e->env_tf.tf_regs.reg_eax = 0;  // 设置接收端IPC返回值（系统调用的返回值是通过%eax传递的）
	return 0;
	panic("sys_ipc_try_send not implemented");
}

```
kern/syscall.c: sys_ipc_recv
<br>IPC接收端系统调用，设置进程阻塞在接收状态，直到接收完成自动返回。
```c
static int
sys_ipc_recv(void *dstva)
{
	// LAB 4: Your code here.
	curenv->env_ipc_dstva = (void*)UTOP;
	if(dstva < (void*)UTOP){
		if((uintptr_t)dstva % PGSIZE)
			return -E_INVAL;
		curenv->env_ipc_dstva = dstva;
	}
	curenv->env_ipc_recving = 1;
	curenv->env_status = ENV_NOT_RUNNABLE;
	sched_yield();  // sched_yield不会返回，而是直接返回用户空间
	return 0;       // 不会从这里退出系统调用
	panic("sys_ipc_recv not implemented");
	return 0;
}
```
注：还需要将系统调用处理函数，在kern/syscall.c: syscall函数中与系统调用号绑定

lib/ipc.c: ipc_recv
```c
int32_t
ipc_recv(envid_t *from_env_store, void *pg, int *perm_store)
{
	// LAB 4: Your code here.
	int r = sys_ipc_recv(pg != NULL? pg : (void*)UTOP);
	if (r < 0){
		if(from_env_store != NULL)
			*from_env_store = 0;
		if(perm_store != NULL)
			*perm_store = 0;
		return r;
	}
	
	if(from_env_store != NULL)
		*from_env_store = thisenv->env_ipc_from;
	if(perm_store != NULL)
		*perm_store = thisenv->env_ipc_perm;
	return thisenv->env_ipc_value;
	panic("ipc_recv not implemented");
	return 0;
}
```

lib/ipc.c: ipc_send
```c
void
ipc_send(envid_t to_env, uint32_t val, void *pg, int perm)
{
	// LAB 4: Your code here.
	int r;
	while((r = sys_ipc_try_send(to_env, val, pg!=NULL?pg:(void*)UTOP, perm)) == -E_IPC_NOT_RECV){
		sys_yield();
	}
	if(r < 0){
		panic("sys_ipc_try_send failed, %e", r);
	}
	return;
	panic("ipc_send not implemented");
}
```